## An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale

> Dosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth 16x16 words: Transformers for image recognition at scale[C]. ICLR,2021

## Abstract
Transformer architecture has become the de-facto standard for natural language processing tasks. But its use in computer vision is limited. It is possible to use a pure transformer on sequence of image patches for image classification, without relying on CNN. The Vision Transformer (ViT) performs well on multiple image recognition benchmarks when pre-trained on large amount of data, and requires fewer computational resources than start-of-the-art convolutional networks.

## Introduction

Transformers have become the model of choice in natual language processing (NLP).  However, convolutional architectures remain dominant in computer vision. We split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. 

When trained on mid-size datasets such as ImageNet-1K without strong regularization, the model yield modest accuracies of a few percentage points below ResNets of comparable size.  However, the picture changes if the models are trained on larger datasets. When pre-train on the ImageNet-21K dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks.



## Method

An overview of the model is depicted in `Figure 1`. The standard Transformer receives as input a $1D$ sequence of token embeddings. To handle $2D$ images, we reshape the image $x\in R^{H \times W \times C }$ into a sequece of flattened $2D$ patches $x_p \in R^{N}\times(P^2 \cdot C)$, where $(H, W)$ is the resolution of the original image, $C$ is the number of channels, $(P,P)$ is the resolution of teach image patch, and $N = HW/P^2$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer use constant latent vector size $D$ through all of its layers, so we flatten the patches and map to $D$ dimensions with a trainable linear projection. We refer to the output of this projection as the patch embeddings. 

Similar to BERT's [class] token, we prepend a learnable embedding to the sequence of embedded patches $(Z_0^0)=x_{class}$, whose state at the output of the Transformer encode $(z_L^0)$ serves as the image representation y $(Eq.4)$. Both during pre-training and fine-tuning, a classification head is attached to $z_L^0$. The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time.

Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable $1D$ position embeddings, since we have not observed significant performance gains from using more advanced $2D$-aware position embeddings. The resulting sequence of embedding vectors serves as input to the encoder.

The Transformer encoder consists of alternating layers of multiheaded self-attention (MSA, see Appendix A) and MLP blocks. Layernorm is applied before every block, and residual connection after every block. The MLP contains two layers with a GELU non-linearity.

$$ z_0 = [x_{class}; x_p^1 E; x_p^2 E;...; x_p^N E;]+E_{pos},    E \in R(P^2 \cdot C) \times D    (1) $$ 
$$ z'_\ell = MSA(LN(z_{\ell-1})) + z_{\ell-1}               \ell=1...L  (2)$$
$$ z_\ell = MLP(LN(z'_\ell)) + z'_\ell                      \ell=1...L  (3)$$
$$ y = LN(z_L^0)                                            (4)  $$


### Fine-tuning and higher resolution

We pre-train ViT on large datasets, and fine-tine to downstream dataset. For this, we remove pre-trained prediction head and attach a zero-initialized $D \times K$ feedforward layer, where $K$ is the number of downstream classes. It is often beneficial to fine-tune at higher resolution, we keep the patch size the same, which results in a larger effective sequence length.


![image](https://user-images.githubusercontent.com/69186975/220640023-a0aeebe4-5bae-4ae0-838e-9fd0b21e30ed.png)
<center>Fig1: Model overview. </center>



[[1]-readpaper](https://readpaper.com/pdf-annotate/note?pdfId=4498390710599966721&noteId=643677395550609408)


[[2]-arxiv paper](https://arxiv.org/pdf/2010.11929.pdf)